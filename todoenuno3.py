"""
Buscador y Verificador Sem√°ntico Integrado - VERSI√ìN ALTO VOLUMEN CON IA AVANZADA
Combina b√∫squeda en m√∫ltiples bases de datos cient√≠ficas con an√°lisis sem√°ntico AI de √∫ltima generaci√≥n
Soporta hasta 1000 resultados por base de datos
"""

import streamlit as st
import pandas as pd
import requests
import time
import urllib.parse
from datetime import datetime
import plotly.express as px
import plotly.graph_objects as go
from streamlit_option_menu import option_menu
import io
import xml.etree.ElementTree as ET
import re
from concurrent.futures import ThreadPoolExecutor, as_completed, ProcessPoolExecutor
import os
import pathlib
from typing import List, Dict, Tuple, Optional
import zipfile
import random
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from collections import Counter
import nltk
import warnings
warnings.filterwarnings('ignore')
from deep_translator import GoogleTranslator
import smtplib
from email.mime.multipart import MIMEMultipart
from email.mime.text import MIMEText
from email.mime.base import MIMEBase
from email import encoders
from email.utils import formatdate
import ssl
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# Configuraci√≥n de la p√°gina
st.set_page_config(
    page_title="üî¨ Buscador y Verificador Sem√°ntico Integrado",
    page_icon="üî¨",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ============================================================================
# CONFIGURACI√ìN DE CORREO
# ============================================================================

class EmailConfig:
    def __init__(self):
        # Configuraci√≥n SMTP (usando variables de entorno o secrets de Streamlit)
        self.SMTP_SERVER = st.secrets.get("smtp_server", "smtp.gmail.com")
        self.SMTP_PORT = st.secrets.get("smtp_port", 587)
        self.EMAIL_USER = st.secrets.get("email_user", "")
        self.EMAIL_PASSWORD = st.secrets.get("email_password", "")
        self.MAX_FILE_SIZE_MB = 10
        
        # Verificar si tenemos configuraci√≥n de correo
        self.available = all([
            self.SMTP_SERVER,
            self.SMTP_PORT,
            self.EMAIL_USER,
            self.EMAIL_PASSWORD
        ])

EMAIL_CONFIG = EmailConfig()

def validate_email(email):
    """Valida el formato de un email"""
    if not email or email == "":
        return False
    pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$'
    return re.match(pattern, email) is not None

def enviar_correo(destinatario, asunto, mensaje_html=None, mensaje_texto=None, archivos=None):
    """
    Env√≠a correo electr√≥nico con formato HTML y texto plano
    
    Args:
        destinatario: Email del destinatario
        asunto: Asunto del correo
        mensaje_html: Mensaje en formato HTML
        mensaje_texto: Mensaje en texto plano (alternativa)
        archivos: Lista de diccionarios con {'nombre': 'archivo.pdf', 'contenido': bytes}
    
    Returns:
        bool: True si se envi√≥ correctamente
    """
    if not EMAIL_CONFIG.available:
        st.error("Configuraci√≥n de correo no disponible. Verifica los secrets de la aplicaci√≥n.")
        return False
    
    if not validate_email(destinatario):
        st.error("Direcci√≥n de correo no v√°lida")
        return False
    
    if not asunto or (not mensaje_html and not mensaje_texto):
        st.error("Faltan datos requeridos para enviar el correo")
        return False
    
    try:
        msg = MIMEMultipart('alternative')
        msg['From'] = EMAIL_CONFIG.EMAIL_USER
        msg['To'] = destinatario
        msg['Subject'] = asunto
        msg['Date'] = formatdate(localtime=True)
        
        # Adjuntar versi√≥n texto plano (si existe)
        if mensaje_texto:
            msg.attach(MIMEText(mensaje_texto, 'plain'))
        
        # Adjuntar versi√≥n HTML (si existe)
        if mensaje_html:
            msg.attach(MIMEText(mensaje_html, 'html'))
        
        # Adjuntar archivos
        if archivos:
            for archivo in archivos:
                if len(archivo['contenido']) > EMAIL_CONFIG.MAX_FILE_SIZE_MB * 1024 * 1024:
                    st.warning(f"El archivo {archivo['nombre']} excede el tama√±o m√°ximo de {EMAIL_CONFIG.MAX_FILE_SIZE_MB}MB y no ser√° enviado")
                    continue
                
                part = MIMEBase('application', 'octet-stream')
                part.set_payload(archivo['contenido'])
                encoders.encode_base64(part)
                part.add_header('Content-Disposition', f'attachment; filename="{archivo["nombre"]}"')
                msg.attach(part)
        
        # Enviar correo
        context = ssl.create_default_context()
        
        with smtplib.SMTP(EMAIL_CONFIG.SMTP_SERVER, EMAIL_CONFIG.SMTP_PORT) as server:
            server.starttls(context=context)
            server.login(EMAIL_CONFIG.EMAIL_USER, EMAIL_CONFIG.EMAIL_PASSWORD)
            server.send_message(msg)
        
        return True
        
    except Exception as e:
        st.error(f"Error enviando correo: {str(e)}")
        return False

# ============================================================================
# CONFIGURACI√ìN NLTK
# ============================================================================

def setup_nltk():
    """Configura NLTK y descarga los recursos necesarios"""
    try:
        nltk.data.find('tokenizers/punkt')
    except LookupError:
        try:
            nltk.download('punkt')
        except:
            st.warning("No se pudo descargar punkt. Usando tokenizaci√≥n alternativa.")
            return False
    
    try:
        nltk.data.find('corpora/stopwords')
    except LookupError:
        try:
            nltk.download('stopwords')
        except:
            st.warning("No se pudieron descargar stopwords. Usando lista b√°sica.")
            return False
    
    try:
        from nltk.corpus import stopwords
        from nltk.stem import SnowballStemmer
        return True
    except:
        return False

# Configurar NLTK al inicio
NLTK_READY = setup_nltk()

if NLTK_READY:
    from nltk.corpus import stopwords
    from nltk.stem import SnowballStemmer
    from nltk.tokenize import sent_tokenize

# ============================================================================
# ESTILOS CSS
# ============================================================================

st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        color: #1E88E5;
        text-align: center;
        margin-bottom: 1rem;
        font-weight: bold;
    }
    .sub-header {
        font-size: 1.2rem;
        color: #424242;
        text-align: center;
        margin-bottom: 2rem;
    }
    .result-card {
        background-color: #f8f9fa;
        border-radius: 10px;
        padding: 1.5rem;
        margin-bottom: 1rem;
        border-left: 5px solid #1E88E5;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
    }
    .verdict-assert {
        background: linear-gradient(135deg, #4CAF50, #2E7D32);
        color: white;
        padding: 0.5rem 1rem;
        border-radius: 20px;
        display: inline-block;
        font-weight: bold;
    }
    .verdict-reject {
        background: linear-gradient(135deg, #f44336, #b71c1c);
        color: white;
        padding: 0.5rem 1rem;
        border-radius: 20px;
        display: inline-block;
        font-weight: bold;
    }
    .verdict-inconclusive {
        background: linear-gradient(135deg, #ff9800, #bf360c);
        color: white;
        padding: 0.5rem 1rem;
        border-radius: 20px;
        display: inline-block;
        font-weight: bold;
    }
    .badge-pubmed { 
        background-color: #1E88E5; 
        color: white; 
        padding: 0.2rem 0.8rem; 
        border-radius: 15px; 
        font-size: 0.8rem;
        display: inline-block;
        margin-bottom: 0.5rem;
    }
    .badge-crossref { 
        background-color: #43A047; 
        color: white; 
        padding: 0.2rem 0.8rem; 
        border-radius: 15px; 
        font-size: 0.8rem;
        display: inline-block;
        margin-bottom: 0.5rem;
    }
    .badge-openalex { 
        background-color: #FDD835; 
        color: #333; 
        padding: 0.2rem 0.8rem; 
        border-radius: 15px; 
        font-size: 0.8rem;
        display: inline-block;
        margin-bottom: 0.5rem;
    }
    .badge-semantic { 
        background-color: #E53935; 
        color: white; 
        padding: 0.2rem 0.8rem; 
        border-radius: 15px; 
        font-size: 0.8rem;
        display: inline-block;
        margin-bottom: 0.5rem;
    }
    .badge-doaj { 
        background-color: #9C27B0; 
        color: white; 
        padding: 0.2rem 0.8rem; 
        border-radius: 15px; 
        font-size: 0.8rem;
        display: inline-block;
        margin-bottom: 0.5rem;
    }
    .badge-europepmc { 
        background-color: #FF5722; 
        color: white; 
        padding: 0.2rem 0.8rem; 
        border-radius: 15px; 
        font-size: 0.8rem;
        display: inline-block;
        margin-bottom: 0.5rem;
    }
    .section-badge {
        display: inline-block;
        padding: 0.2rem 0.5rem;
        border-radius: 12px;
        font-size: 0.7rem;
        font-weight: bold;
        margin-right: 0.5rem;
    }
    .section-abstract { background-color: #e1f5fe; color: #01579b; }
    .section-introduction { background-color: #fff3e0; color: #bf360c; }
    .section-methods { background-color: #e8f5e8; color: #1b5e20; }
    .section-results { background-color: #f3e5f5; color: #4a148c; }
    .section-discussion { background-color: #fff8e1; color: #ff6f00; }
    .section-conclusion { background-color: #ffebee; color: #b71c1c; }
    
    .progress-container {
        background-color: #f0f2f6;
        border-radius: 10px;
        padding: 1rem;
        margin: 1rem 0;
    }
    .stats-box {
        background-color: #e8f5e9;
        border-left: 5px solid #2e7d32;
        padding: 1rem;
        border-radius: 5px;
        margin: 0.5rem 0;
    }
    .evidence-box {
        background-color: #fff3e0;
        border-left: 5px solid #ff9800;
        padding: 0.5rem;
        margin: 0.2rem 0;
        font-size: 0.85rem;
    }
    .warning-box {
        background-color: #fff3cd;
        color: #856404;
        padding: 1rem;
        border-radius: 5px;
        border-left: 5px solid #ffc107;
        margin: 1rem 0;
    }
    .assistant-box {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        padding: 2rem;
        border-radius: 15px;
        margin: 1rem 0;
    }
    .assistant-step {
        background-color: rgba(255,255,255,0.1);
        border-radius: 10px;
        padding: 1rem;
        margin: 1rem 0;
    }
    .tip-box {
        background-color: #e3f2fd;
        border-left: 5px solid #2196f3;
        padding: 1rem;
        border-radius: 5px;
        margin: 1rem 0;
        font-size: 0.95rem;
    }
    .email-box {
        background-color: #fce4e4;
        border-left: 5px solid #e53935;
        padding: 1rem;
        border-radius: 5px;
        margin: 1rem 0;
    }
</style>
""", unsafe_allow_html=True)


# ============================================================================
# TOKENIZADOR ALTERNATIVO
# ============================================================================

def simple_sent_tokenize(text: str) -> List[str]:
    """Tokenizador simple de oraciones como fallback"""
    abbreviations = ['Dr.', 'Dra.', 'Prof.', 'vs.', 'Fig.', 'Eq.', 'et al.', 
                    'i.e.', 'e.g.', 'p.ej.', 'vol.', 'no.', 'pp.', 'eds.']
    
    for i, abbr in enumerate(abbreviations):
        text = text.replace(abbr, f"ABBR{i}")
    
    sentences = re.split(r'[.!?]+', text)
    
    for i, abbr in enumerate(abbreviations):
        sentences = [s.replace(f"ABBR{i}", abbr) for s in sentences]
    
    sentences = [s.strip() for s in sentences if len(s.strip()) > 30]
    
    return sentences


# ============================================================================
# CLASE DE TRADUCCI√ìN
# ============================================================================

class TranslationManager:
    """Maneja traducciones con cach√©"""
    
    def __init__(self):
        self.cache = {}
        try:
            self.translator = GoogleTranslator(source='auto', target='es')
            self.translator_en = GoogleTranslator(source='auto', target='en')
            self.available = True
        except:
            self.available = False
    
    def translate_to_spanish(self, text: str) -> str:
        if not text or len(text) > 4000 or not self.available:
            return text
        cache_key = f"es_{text[:100]}"
        if cache_key in self.cache:
            return self.cache[cache_key]
        try:
            result = self.translator.translate(text[:3500])
            self.cache[cache_key] = result
            return result
        except:
            return text
    
    def translate_to_english(self, text: str) -> str:
        if not text or len(text) > 4000 or not self.available:
            return text
        cache_key = f"en_{text[:100]}"
        if cache_key in self.cache:
            return self.cache[cache_key]
        try:
            result = self.translator_en.translate(text[:3500])
            self.cache[cache_key] = result
            return result
        except:
            return text


# ============================================================================
# ASISTENTE DE CONSTRUCCI√ìN DE CONJETURAS (EXACTAMENTE IGUAL)
# ============================================================================

class HypothesisAssistant:
    """Asistente para ayudar al usuario a construir conjeturas bien formadas"""
    
    def __init__(self):
        self.translator = TranslationManager()
        
        # Plantillas de conjeturas comunes - CORREGIDAS
        self.templates = {
            "causal": {
                "es": "El/La {sujeto} {verbo} {efecto} en {poblacion}",
                "en": "The {subject} {verb} {effect} in {population}",
                "description": "Relaci√≥n causal directa",
                "verbs_es": ["causa", "produce", "induce", "provoca", "genera"],
                "verbs_en": ["causes", "produces", "induces", "provokes", "generates"]
            },
            "asociacion": {
                "es": "Existe una asociaci√≥n entre {sujeto} y {efecto} en {poblacion}",
                "en": "There is an association between {subject} and {effect} in {population}",
                "description": "Asociaci√≥n o correlaci√≥n",
                "verbs_es": ["se asocia con", "se relaciona con", "se correlaciona con"],
                "verbs_en": ["is associated with", "is related to", "is correlated with"]
            },
            "riesgo": {
                "es": "El/La {sujeto} aumenta el riesgo de {efecto} en {poblacion}",
                "en": "The {subject} increases the risk of {effect} in {population}",
                "description": "Factor de riesgo",
                "verbs_es": ["aumenta el riesgo de", "incrementa la probabilidad de", "es factor de riesgo para"],
                "verbs_en": ["increases the risk of", "is a risk factor for"]
            },
            "prevencion": {
                "es": "El/La {sujeto} reduce la incidencia de {efecto} en {poblacion}",
                "en": "The {subject} reduces the incidence of {effect} in {population}",
                "description": "Factor protector o preventivo",
                "verbs_es": ["reduce", "disminuye", "previene", "protege contra"],
                "verbs_en": ["reduces", "decreases", "prevents", "protects against"]
            },
            "efectividad": {
                "es": "El/La {sujeto} es efectivo para tratar {efecto} en {poblacion}",
                "en": "The {subject} is effective in treating {effect} in {population}",
                "description": "Efectividad terap√©utica",
                "verbs_es": ["es efectivo para", "es eficaz para", "demuestra eficacia en"],
                "verbs_en": ["is effective in", "demonstrates efficacy in"]
            }
        }
        
        # Ejemplos predefinidos
        self.examples = [
            {
                "name": "Ticagrelor y disnea",
                "sujeto": "ticagrelor",
                "efecto": "disnea",
                "poblacion": "pacientes con cardiopat√≠a isqu√©mica",
                "tipo": "causal",
                "verbo": "causa",
                "hypothesis": "El ticagrelor causa disnea como efecto secundario en pacientes con cardiopat√≠a isqu√©mica"
            },
            {
                "name": "COVID-19 y da√±o mioc√°rdico",
                "sujeto": "infecci√≥n por COVID-19",
                "efecto": "da√±o mioc√°rdico",
                "poblacion": "pacientes hospitalizados",
                "tipo": "causal",
                "verbo": "causa",
                "hypothesis": "La infecci√≥n por COVID-19 causa da√±o mioc√°rdico en pacientes hospitalizados"
            },
            {
                "name": "Ejercicio y diabetes",
                "sujeto": "ejercicio f√≠sico regular",
                "efecto": "diabetes tipo 2",
                "poblacion": "adultos con sobrepeso",
                "tipo": "prevencion",
                "verbo": "reduce la incidencia de",
                "hypothesis": "El ejercicio f√≠sico regular reduce la incidencia de diabetes tipo 2 en adultos con sobrepeso"
            }
        ]
    
    def get_available_verbs(self, template_type: str, language: str = "es") -> List[str]:
        """Obtiene verbos disponibles para un tipo de plantilla"""
        if template_type in self.templates:
            return self.templates[template_type].get(f"verbs_{language}", [])
        return []
    
    def build_hypothesis(self, subject: str, effect: str, population: str, 
                        template_type: str, verb: str = None) -> Dict:
        """
        Construye una conjetura bien formada en espa√±ol e ingl√©s
        
        Args:
            subject: Sujeto de estudio (ej: "ticagrelor", "ejercicio")
            effect: Efecto observado (ej: "disnea", "mejor√≠a")
            population: Poblaci√≥n de estudio
            template_type: Tipo de plantilla
            verb: Verbo espec√≠fico (opcional)
        
        Returns:
            Dict con conjeturas en espa√±ol e ingl√©s
        """
        if template_type not in self.templates:
            return {"es": "", "en": ""}
        
        template = self.templates[template_type]
        
        # Si no se especifica verbo, usar el primero de la lista
        if not verb and template.get(f"verbs_es"):
            verb = template[f"verbs_es"][0]
        
        # Construir en espa√±ol
        if verb:
            # Para plantillas que tienen el verbo como parte de la frase
            if "{verbo}" in template["es"]:
                hypothesis_es = template["es"].format(
                    sujeto=subject,
                    verbo=verb,
                    efecto=effect,
                    poblacion=population
                )
            else:
                # Para plantillas que ya incluyen el verbo en la estructura
                hypothesis_es = template["es"].format(
                    sujeto=subject,
                    efecto=effect,
                    poblacion=population
                )
        else:
            hypothesis_es = template["es"].format(
                sujeto=subject,
                efecto=effect,
                poblacion=population
            )
        
        # Traducir al ingl√©s
        hypothesis_en = self.translator.translate_to_english(hypothesis_es)
        
        # Construcci√≥n directa en ingl√©s para mayor precisi√≥n
        subject_en = self.translator.translate_to_english(subject)
        effect_en = self.translator.translate_to_english(effect)
        population_en = self.translator.translate_to_english(population)
        
        # Encontrar verbo en ingl√©s correspondiente
        verb_en = ""
        if verb and template.get(f"verbs_es") and template.get(f"verbs_en"):
            try:
                verb_idx = template["verbs_es"].index(verb)
                verbs_en = template["verbs_en"]
                verb_en = verbs_en[verb_idx] if verb_idx < len(verbs_en) else verbs_en[0]
            except ValueError:
                # Si el verbo no est√° en la lista, usar traducci√≥n
                verb_en = self.translator.translate_to_english(verb)
        
        # Construcci√≥n directa en ingl√©s
        if verb_en and "{verb}" in template["en"]:
            hypothesis_en_direct = template["en"].format(
                subject=subject_en,
                verb=verb_en,
                effect=effect_en,
                population=population_en
            )
        else:
            hypothesis_en_direct = template["en"].format(
                subject=subject_en,
                effect=effect_en,
                population=population_en
            )
        
        return {
            "es": hypothesis_es,
            "en": hypothesis_en,
            "en_direct": hypothesis_en_direct,
            "subject": subject,
            "effect": effect,
            "population": population,
            "type": template_type,
            "type_description": template["description"],
            "verb": verb,
            "verb_en": verb_en
        }
    
    def render_assistant_ui(self):
        """Renderiza la interfaz del asistente en Streamlit"""
        
        with st.expander("ü§ñ ASISTENTE DE CONJETURAS - Ayuda a construir tu hip√≥tesis", expanded=False):
            st.markdown('<div class="assistant-box">', unsafe_allow_html=True)
            st.markdown("### üéØ Construye tu conjetura cient√≠fica")
            st.markdown("Completa los siguientes campos para generar una hip√≥tesis bien formada:")
            
            col1, col2 = st.columns(2)
            
            with col1:
                # Selecci√≥n de ejemplo o entrada manual
                example_option = st.selectbox(
                    "üìã Cargar ejemplo:",
                    ["Personalizado"] + [e["name"] for e in self.examples],
                    key="example_selector"
                )
                
                # Campos del formulario
                subject = st.text_input(
                    "üß™ Sujeto/Intervenci√≥n:",
                    value="",
                    placeholder="Ej: ticagrelor, ejercicio, vacuna, f√°rmaco X",
                    help="¬øQu√© elemento est√°s estudiando?"
                )
                
                effect = st.text_input(
                    "üìä Efecto/Desenlace:",
                    value="",
                    placeholder="Ej: disnea, mejor√≠a, mortalidad, efecto secundario",
                    help="¬øQu√© efecto esperas observar?"
                )
                
                population = st.text_input(
                    "üë• Poblaci√≥n:",
                    value="",
                    placeholder="Ej: pacientes con cardiopat√≠a, adultos mayores, ni√±os",
                    help="¬øEn qu√© poblaci√≥n?"
                )
            
            with col2:
                # Tipo de relaci√≥n
                template_type = st.selectbox(
                    "üîÑ Tipo de relaci√≥n:",
                    options=list(self.templates.keys()),
                    format_func=lambda x: f"{x} - {self.templates[x]['description']}",
                    key="template_type"
                )
                
                # Verbos disponibles seg√∫n el tipo
                available_verbs = self.get_available_verbs(template_type)
                if available_verbs:
                    verb = st.selectbox(
                        "üî§ Verbo/relaci√≥n:",
                        options=available_verbs,
                        key="selected_verb"
                    )
                else:
                    verb = None
                
                st.markdown("---")
                st.markdown("##### üí° Sugerencias:")
                if template_type == "causal":
                    st.info("Usa verbos fuertes como 'causa', 'induce' para relaciones causales directas")
                elif template_type == "asociacion":
                    st.info("Usa 'se asocia con', 'se relaciona con' para correlaciones sin causalidad establecida")
                elif template_type == "riesgo":
                    st.info("Adecuado cuando el sujeto aumenta la probabilidad del efecto")
                elif template_type == "prevencion":
                    st.info("Usa cuando el sujeto reduce el riesgo o protege contra el efecto")
                elif template_type == "efectividad":
                    st.info("Ideal para evaluar intervenciones terap√©uticas")
            
            # Bot√≥n para generar
            if st.button("‚ú® GENERAR CONJETURA", type="primary", use_container_width=True):
                if subject and effect and population:
                    # Construir hip√≥tesis
                    hypothesis_data = self.build_hypothesis(
                        subject=subject,
                        effect=effect,
                        population=population,
                        template_type=template_type,
                        verb=verb
                    )
                    
                    # Mostrar resultado
                    st.markdown("---")
                    st.markdown("### üìù CONJETURA GENERADA")
                    
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        st.markdown("**üá™üá∏ Espa√±ol:**")
                        st.success(hypothesis_data["es"])
                        
                        # Bot√≥n para usar esta hip√≥tesis - CORREGIDO
                        if st.button("üìå Usar esta hip√≥tesis en espa√±ol", key="use_es"):
                            st.session_state['hypothesis'] = hypothesis_data["es"]
                            st.session_state['hypothesis_en'] = hypothesis_data["en_direct"]
                            st.rerun()
                    
                    with col2:
                        st.markdown("**üá¨üáß Ingl√©s (para b√∫squeda):**")
                        st.info(hypothesis_data["en_direct"])
                        
                        if st.button("üìå Usar esta hip√≥tesis (ingl√©s)", key="use_en"):
                            st.session_state['hypothesis'] = hypothesis_data["es"]
                            st.session_state['hypothesis_en'] = hypothesis_data["en_direct"]
                            st.rerun()
                    
                    # Guardar en session state
                    st.session_state['last_hypothesis_data'] = hypothesis_data
                    
                else:
                    st.warning("‚ö†Ô∏è Completa todos los campos para generar la conjetura")
            
            # Cargar ejemplo si se selecciona
            if example_option != "Personalizado":
                example = next((e for e in self.examples if e["name"] == example_option), None)
                if example:
                    st.markdown("---")
                    st.markdown("### üìã Ejemplo cargado:")
                    st.info(f"**Hip√≥tesis:** {example['hypothesis']}")
                    
                    col1, col2 = st.columns(2)
                    with col1:
                        if st.button("üìå Usar este ejemplo", key="use_example"):
                            st.session_state['hypothesis'] = example['hypothesis']
                            st.session_state['query'] = f'("{example["sujeto"]}" AND "{example["efecto"]}")'
                            # Tambi√©n guardar traducci√≥n aproximada
                            translator = TranslationManager()
                            st.session_state['hypothesis_en'] = translator.translate_to_english(example['hypothesis'])
                            st.rerun()
            
            st.markdown('</div>', unsafe_allow_html=True)
            
            # Consejos adicionales
            st.markdown('<div class="tip-box">', unsafe_allow_html=True)
            st.markdown("""
            **üí° Consejos para una buena conjetura:**
            
            1. **S√© espec√≠fico**: Cuanto m√°s espec√≠fico, mejor podr√° verificar la evidencia
            2. **Define claramente**: El sujeto, efecto y poblaci√≥n deben estar claramente definidos
            3. **Usa terminolog√≠a m√©dica**: Los art√≠culos cient√≠ficos usan t√©rminos MeSH estandarizados
            4. **Considera la direcci√≥n**: ¬øEs causalidad, asociaci√≥n, riesgo o protecci√≥n?
            5. **Poblaci√≥n relevante**: Especifica edad, condici√≥n, contexto cuando sea relevante
            """)
            st.markdown('</div>', unsafe_allow_html=True)
    
    def translate_hypothesis_for_search(self, hypothesis_es: str) -> str:
        """
        Traduce una hip√≥tesis en espa√±ol a ingl√©s para b√∫squeda
        """
        if not hypothesis_es:
            return ""
        
        # Si ya tenemos una traducci√≥n en session state, usarla
        if 'hypothesis_en' in st.session_state and st.session_state['hypothesis_en']:
            return st.session_state['hypothesis_en']
        
        # Traducir
        return self.translator.translate_to_english(hypothesis_es)


# ============================================================================
# VERIFICADOR SEM√ÅNTICO AVANZADO CON IA (MEJORADO)
# ============================================================================

class AdvancedSemanticVerifier:
    """
    Verificador sem√°ntico avanzado con t√©cnicas de IA modernas
    - An√°lisis sem√°ntico profundo
    - Detecci√≥n de matices ling√º√≠sticos
    - Evaluaci√≥n de calidad metodol√≥gica
    - Ponderaci√≥n por secciones
    """
    
    def __init__(self):
        # Configurar NLTK si est√° disponible
        if NLTK_READY:
            self.stop_words_es = set(stopwords.words('spanish'))
            self.stop_words_en = set(stopwords.words('english'))
            self.stemmer = SnowballStemmer('spanish')
        else:
            # Stop words b√°sicas como fallback
            self.stop_words_es = {'el', 'la', 'los', 'las', 'de', 'del', 'y', 'o', 
                                  'a', 'en', 'por', 'para', 'con', 'sin', 'sobre'}
            self.stop_words_en = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on',
                                 'at', 'to', 'for', 'with', 'without', 'by'}
            self.stemmer = None
        
        self.translator = TranslationManager()
        
        # ====================================================================
        # PESOS POR SECCI√ìN DEL ART√çCULO (basado en importancia cient√≠fica)
        # ====================================================================
        self.section_weights = {
            'abstract': 1.5,      # Resumen ejecutivo
            'introduction': 0.6,   # Contexto, no evidencia directa
            'methods': 0.4,        # Metodolog√≠a, no resultados
            'results': 1.8,        # Resultados, alta importancia
            'discussion': 1.5,     # Discusi√≥n, interpretaci√≥n
            'conclusion': 1.8,     # Conclusi√≥n, muy importante
            'unknown': 1.0
        }
        
        # ====================================================================
        # KEYWORDS PARA DETECTAR SECCIONES
        # ====================================================================
        self.section_keywords = {
            'abstract': ['abstract', 'resumen', 'background', 'objective', 'objetivo', 
                        'methods', 'm√©todos', 'results', 'resultados'],
            'introduction': ['introduction', 'introducci√≥n', 'background', 'antecedentes'],
            'methods': ['methods', 'm√©todos', 'methodology', 'metodolog√≠a', 
                       'material and methods', 'material y m√©todos'],
            'results': ['results', 'resultados', 'findings', 'hallazgos'],
            'discussion': ['discussion', 'discusi√≥n'],
            'conclusion': ['conclusion', 'conclusiones', 'concluding', 'conclusi√≥n']
        }
        
        # ====================================================================
        # DETECTOR DE MATICES LING√ú√çSTICOS (NIVELES DE CERTEZA)
        # ====================================================================
        self.certainty_levels = {
            'definitive': {
                'terms': ['demonstrates', 'proves', 'establishes', 'conclusive', 
                         'definitively', 'undoubtedly', 'certainly'],
                'weight': 1.5
            },
            'strong': {
                'terms': ['strongly suggests', 'provides evidence', 'indicates',
                         'shows', 'reveals', 'confirms'],
                'weight': 1.3
            },
            'moderate': {
                'terms': ['suggests', 'supports', 'consistent with', 'implies',
                         'appears to', 'seems to'],
                'weight': 1.0
            },
            'weak': {
                'terms': ['may', 'might', 'could', 'possibly', 'potentially',
                         'preliminary', 'tentative', 'speculative'],
                'weight': 0.5
            }
        }
        
        # ====================================================================
        # PATRONES DE NEGACI√ìN
        # ====================================================================
        self.negation_patterns = [
            r'no\s+(hay|existe|se\s+observ√≥|se\s+encontr√≥)\s+(asociaci√≥n|relaci√≥n|correlaci√≥n)',
            r'no\s+(se\s+)?(demostr√≥|evidenci√≥|confirm√≥)\s+',
            r'no\s+(fue|result√≥)\s+(significativo|estad√≠sticamente\s+significativo)',
            r'p\s*[>=]\s*0\.0[5-9]',
            r'p\s*>\s*0\.05',
            r'not\s+(associated|related|correlated)',
            r'no\s+(significant|statistically\s+significant)'
        ]
        
        # ====================================================================
        # TIPOS DE RELACI√ìN (con pesos)
        # ====================================================================
        self.relation_types = {
            'causal': {
                'terms': ['causes', 'leads to', 'results in', 'induced by',
                         'due to', 'attributed to', 'responsible for'],
                'weight': 1.4
            },
            'association': {
                'terms': ['associated with', 'related to', 'linked to',
                         'correlated with', 'connection between'],
                'weight': 1.2
            },
            'risk': {
                'terms': ['risk factor', 'increases risk', 'higher risk',
                         'elevated risk', 'predisposes to'],
                'weight': 1.3
            },
            'protective': {
                'terms': ['protective', 'reduces risk', 'decreases risk',
                         'prevents', 'lowers risk'],
                'weight': 1.1
            }
        }
        
        # ====================================================================
        # EVALUACI√ìN DE CALIDAD DE ESTUDIOS
        # ====================================================================
        self.study_type_weights = {
            'meta-analysis': 2.5,
            'systematic review': 2.3,
            'randomized controlled trial': 2.0,
            'cohort study': 1.6,
            'case-control study': 1.4,
            'cross-sectional study': 1.2,
            'case series': 0.8,
            'case report': 0.5,
            'editorial': 0.3,
            'letter': 0.2,
            'comment': 0.2
        }
        
        self.study_type_patterns = {
            'meta-analysis': ['meta-analysis', 'meta analysis', 'metaanalysis'],
            'systematic review': ['systematic review', 'systematic literature review'],
            'randomized controlled trial': ['randomized controlled trial', 'randomised controlled trial', 'rct'],
            'cohort study': ['cohort study', 'cohort analysis', 'prospective cohort', 'retrospective cohort'],
            'case-control study': ['case-control', 'case control'],
            'cross-sectional study': ['cross-sectional', 'cross sectional'],
            'case series': ['case series'],
            'case report': ['case report']
        }
        
        # ====================================================================
        # PESOS FINALES PARA VOTACI√ìN
        # ====================================================================
        self.vote_weights = {
            'causal': 2.2,
            'risk': 2.0,
            'association': 1.8,
            'protective': 1.5,
            'statistical': 1.8,
            'moderate': 1.0,
            'strong_negation': -1.5,
            'statistical_negation': -1.8
        }
        
        self.UMBRAL_RELEVANCIA = 0.1
    
    # ========================================================================
    # M√âTODOS DE AN√ÅLISIS DE TEXTO
    # ========================================================================
    
    def detect_section(self, text_block: str) -> str:
        """Detecta la secci√≥n del art√≠culo basado en keywords"""
        text_lower = text_block.lower()[:500]
        
        for section, keywords in self.section_keywords.items():
            for keyword in keywords:
                if keyword in text_lower:
                    return section
        
        return 'unknown'
    
    def extract_key_terms(self, hypothesis: str) -> List[str]:
        """Extrae t√©rminos clave de la hip√≥tesis con expansi√≥n sem√°ntica"""
        words = re.findall(r'\b[a-zA-Z√°√©√≠√≥√∫√±√º]+\b', hypothesis.lower())
        
        filtered_words = []
        for word in words:
            if len(word) > 3 and word not in self.stop_words_es and word not in self.stop_words_en:
                if self.stemmer:
                    word = self.stemmer.stem(word)
                filtered_words.append(word)
        
        # A√±adir bigramas relevantes (frases de dos palabras)
        bigrams = re.findall(r'\b[a-zA-Z√°√©√≠√≥√∫√±√º]+\s+[a-zA-Z√°√©√≠√≥√∫√±√º]+\b', hypothesis.lower())
        for bigram in bigrams:
            if all(len(w) > 3 for w in bigram.split()):
                filtered_words.append(bigram.replace(' ', '_'))
        
        return list(set(filtered_words))
    
    def analyze_sentence_deep(self, sentence: str) -> Dict:
        """
        An√°lisis profundo de una oraci√≥n con detecci√≥n de:
        - Tipo de relaci√≥n
        - Nivel de certeza
        - Negaci√≥n
        - Fuerza de la evidencia
        """
        sentence_lower = sentence.lower()
        
        # Detectar negaci√≥n
        has_negation = False
        for pattern in self.negation_patterns:
            if re.search(pattern, sentence_lower, re.IGNORECASE):
                has_negation = True
                break
        
        # Detectar nivel de certeza
        certainty = 'unknown'
        certainty_weight = 0.5
        for level, data in self.certainty_levels.items():
            if any(term in sentence_lower for term in data['terms']):
                certainty = level
                certainty_weight = data['weight']
                break
        
        # Detectar tipo de relaci√≥n
        relation = 'unknown'
        relation_weight = 0.5
        for rel_type, data in self.relation_types.items():
            if any(term in sentence_lower for term in data['terms']):
                relation = rel_type
                relation_weight = data['weight']
                break
        
        # Calcular fuerza de la evidencia
        strength = relation_weight * certainty_weight
        if has_negation:
            strength *= -1  # La negaci√≥n invierte la direcci√≥n
        
        return {
            'has_negation': has_negation,
            'certainty': certainty,
            'certainty_weight': certainty_weight,
            'relation': relation,
            'relation_weight': relation_weight,
            'strength': strength,
            'direction': -1 if has_negation else 1
        }
    
    def assess_study_quality(self, text: str) -> Dict:
        """Eval√∫a la calidad metodol√≥gica del estudio"""
        text_lower = text.lower()
        
        # Detectar tipo de estudio
        study_type = 'unknown'
        study_weight = 1.0
        for stype, patterns in self.study_type_patterns.items():
            if any(p in text_lower for p in patterns):
                study_type = stype
                study_weight = self.study_type_weights.get(stype, 1.0)
                break
        
        # Detectar tama√±o de muestra
        sample_size = None
        sample_match = re.search(r'n\s*[=:]\s*(\d+)', text_lower)
        if sample_match:
            sample_size = int(sample_match.group(1))
            if sample_size > 1000:
                study_weight *= 1.2
            elif sample_size > 500:
                study_weight *= 1.1
            elif sample_size < 50:
                study_weight *= 0.7
            elif sample_size < 100:
                study_weight *= 0.8
        
        # Detectar factores de calidad adicionales
        quality_factors = []
        if re.search(r'(multicenter|multi-center)', text_lower):
            study_weight *= 1.1
            quality_factors.append('multicenter')
        if re.search(r'(double-blind|double blind|randomized)', text_lower):
            study_weight *= 1.2
            quality_factors.append('rigorous')
        if re.search(r'(prospective)', text_lower):
            study_weight *= 1.1
            quality_factors.append('prospective')
        
        return {
            'study_type': study_type,
            'type_weight': study_weight,
            'quality_factors': quality_factors,
            'sample_size': sample_size
        }
    
    def calculate_relevance(self, sentence: str, hypothesis_terms: List[str]) -> float:
        """Calcula relevancia de la oraci√≥n respecto a la hip√≥tesis usando TF-IDF"""
        if not hypothesis_terms:
            return 0.0
        
        sentence_lower = sentence.lower()
        
        # Coincidencia exacta de t√©rminos
        matches = sum(1 for term in hypothesis_terms if term in sentence_lower)
        
        # Coincidencia de ra√≠ces (stemming)
        if self.stemmer:
            sentence_stems = [self.stemmer.stem(w) for w in sentence_lower.split()]
            hypothesis_stems = [self.stemmer.stem(term.replace('_', ' ')) for term in hypothesis_terms]
            stem_matches = sum(1 for stem in hypothesis_stems if any(stem in s for s in sentence_stems))
            matches = max(matches, stem_matches)
        
        score = matches / len(hypothesis_terms)
        
        # Bonus por proximidad de t√©rminos
        if matches >= 2:
            positions = []
            for term in hypothesis_terms:
                pos = sentence_lower.find(term)
                if pos != -1:
                    positions.append(pos)
            
            if len(positions) >= 2:
                positions.sort()
                if positions[-1] - positions[0] < 100:
                    score += 0.2
        
        return min(score, 1.0)
    
    def split_sentences(self, text: str) -> List[str]:
        """Divide texto en oraciones"""
        if NLTK_READY:
            try:
                return sent_tokenize(text)
            except:
                return simple_sent_tokenize(text)
        else:
            return simple_sent_tokenize(text)
    
    # ========================================================================
    # M√âTODO PRINCIPAL DE VERIFICACI√ìN
    # ========================================================================
    
    def verify_article_text(self, text: str, hypothesis: str) -> Dict:
        """
        Verifica un art√≠culo completo usando t√©cnicas avanzadas de IA
        
        Args:
            text: Texto completo del art√≠culo
            hypothesis: Hip√≥tesis a verificar (en ingl√©s)
        
        Returns:
            Dict con resultados del an√°lisis
        """
        if not text or len(text.strip()) < 100:
            return {
                'success': False,
                'error': 'Texto insuficiente para an√°lisis',
                'verdict': None
            }
        
        # PASO 1: Extraer t√©rminos clave de la hip√≥tesis
        hypothesis_terms = self.extract_key_terms(hypothesis)
        
        # PASO 2: Evaluar calidad del estudio
        quality = self.assess_study_quality(text)
        
        # PASO 3: Dividir en oraciones
        sentences = self.split_sentences(text)
        
        # PASO 4: Dividir en bloques para detecci√≥n de secciones
        block_size = max(1, len(text) // 10)
        blocks = [text[i:i+block_size] for i in range(0, len(text), block_size)]
        
        # Mapa de secciones
        section_map = {}
        current_section = 'unknown'
        for i, block in enumerate(blocks):
            detected = self.detect_section(block)
            if detected != 'unknown':
                current_section = detected
            section_map[i] = current_section
        
        # PASO 5: Analizar cada oraci√≥n
        evidence_list = []
        section_counts = Counter()
        
        for i, sentence in enumerate(sentences):
            # Determinar secci√≥n
            block_idx = min(i // max(1, len(sentences) // len(blocks)), len(blocks)-1)
            section = section_map.get(block_idx, 'unknown')
            section_weight = self.section_weights.get(section, 1.0)
            
            # Calcular relevancia
            relevance = self.calculate_relevance(sentence, hypothesis_terms)
            
            if relevance > self.UMBRAL_RELEVANCIA:
                # Traducir para mejor an√°lisis
                sentence_en = self.translator.translate_to_english(sentence)
                
                # An√°lisis profundo de la oraci√≥n
                analysis = self.analyze_sentence_deep(sentence_en)
                
                if analysis['direction'] != 0:  # Si hay evidencia (positiva o negativa)
                    evidence = {
                        'sentence': sentence[:200] + '...' if len(sentence) > 200 else sentence,
                        'sentence_en': sentence_en[:200] + '...' if len(sentence_en) > 200 else sentence_en,
                        'section': section,
                        'section_weight': section_weight,
                        'relevance': relevance,
                        'relation': analysis['relation'],
                        'certainty': analysis['certainty'],
                        'direction': analysis['direction'],
                        'strength': abs(analysis['strength']),
                        'has_negation': analysis['has_negation']
                    }
                    
                    evidence_list.append(evidence)
                    section_counts[section] += 1
        
        # PASO 6: Votaci√≥n ponderada
        verdict = self.weighted_vote(evidence_list, quality)
        
        return {
            'success': True,
            'total_sentences': len(sentences),
            'relevant_sentences': len(evidence_list),
            'section_distribution': dict(section_counts),
            'study_quality': quality,
            'evidence': evidence_list[:15],  # Limitar a 15 evidencias para no saturar
            'verdict': verdict,
            'hypothesis_terms': hypothesis_terms
        }
    
    def weighted_vote(self, evidence_list: List[Dict], quality: Dict) -> Dict:
        """
        Sistema de votaci√≥n ponderada con factores:
        - Tipo de relaci√≥n
        - Nivel de certeza
        - Secci√≥n del art√≠culo
        - Calidad del estudio
        - Relevancia
        """
        if not evidence_list:
            return {
                'score': 0,
                'confidence': 0,
                'verdict': 'inconclusive',
                'verdict_text': 'EVIDENCIA NO CONCLUYENTE',
                'support_count': 0,
                'against_count': 0
            }
        
        # Pesos base por tipo de relaci√≥n
        relation_weights = {
            'causal': 2.2,
            'risk': 2.0,
            'association': 1.8,
            'protective': 1.5,
            'unknown': 1.0
        }
        
        # Pesos por nivel de certeza
        certainty_weights = {
            'definitive': 1.5,
            'strong': 1.3,
            'moderate': 1.0,
            'weak': 0.5,
            'unknown': 0.5
        }
        
        weighted_score = 0.0
        total_weight = 0.0
        support_count = 0
        against_count = 0
        strong_evidence = []
        
        for evidence in evidence_list:
            # Peso base por relaci√≥n
            base_weight = relation_weights.get(evidence['relation'], 1.0)
            
            # Peso por certeza
            certainty_weight = certainty_weights.get(evidence['certainty'], 0.5)
            
            # Peso por secci√≥n
            section_weight = evidence['section_weight']
            
            # Peso por relevancia
            relevance_bonus = 1 + evidence['relevance'] * 0.5
            
            # Peso por calidad del estudio
            quality_multiplier = quality['type_weight'] if quality else 1.0
            
            # Peso final
            final_weight = (base_weight * certainty_weight * section_weight * 
                          relevance_bonus * quality_multiplier)
            
            # Aplicar direcci√≥n (positiva o negativa)
            direction = evidence['direction']
            
            if direction > 0:
                support_count += 1
                if certainty_weight > 1.0:
                    strong_evidence.append('support')
            elif direction < 0:
                against_count += 1
                if certainty_weight > 1.0:
                    strong_evidence.append('against')
            
            weighted_score += direction * final_weight
            total_weight += final_weight
        
        if total_weight == 0:
            avg_score = 0
        else:
            avg_score = weighted_score / total_weight
        
        # Normalizar confianza basado en cantidad y calidad de evidencia
        evidence_quality = len(evidence_list) / 20.0  # 20 es el m√°ximo esperado
        strong_ratio = len(strong_evidence) / max(1, len(evidence_list))
        norm_confidence = min(1.0, (evidence_quality + strong_ratio) / 1.5)
        
        # Determinar veredicto con umbrales ajustados
        if avg_score > 0.8:
            verdict = 'strongly_supports'
            text = 'CORROBORA FUERTEMENTE'
        elif avg_score > 0.3:
            verdict = 'supports'
            text = 'CORROBORA'
        elif avg_score > -0.3:
            verdict = 'inconclusive'
            text = 'EVIDENCIA NO CONCLUYENTE'
        elif avg_score > -0.8:
            verdict = 'contradicts'
            text = 'CONTRADICE'
        else:
            verdict = 'strongly_contradicts'
            text = 'CONTRADICE FUERTEMENTE'
        
        return {
            'score': avg_score,
            'confidence': norm_confidence,
            'verdict': verdict,
            'verdict_text': text,
            'support_count': support_count,
            'against_count': against_count,
            'strong_evidence_count': len(strong_evidence)
        }


# ============================================================================
# MOTOR DE B√öSQUEDA CIENT√çFICA (VERSI√ìN ALTO VOLUMEN) - EXACTAMENTE IGUAL
# ============================================================================

class ScientificSearchEngine:
    """
    Motor de b√∫squeda cient√≠fica mejorado para grandes vol√∫menes de resultados
    """
    
    def __init__(self, email: str):
        self.email = email
        self.delay = 0.3
        
    def format_pubmed_query(self, query: str, year_range: tuple = None) -> str:
        """Formatea consultas complejas de PubMed incluyendo rango de a√±os"""
        query = ' '.join(query.split())
        
        mesh_pattern = r'"([^"]+)"\[Mesh\]'
        query = re.sub(mesh_pattern, r'\1[mh]', query)
        
        pubtype_pattern = r'"([^"]+)"\[Publication Type\]'
        query = re.sub(pubtype_pattern, r'\1[pt]', query)
        
        field_mappings = {
            '[Mesh]': '[mh]',
            '[Publication Type]': '[pt]',
            '[Title/Abstract]': '[tiab]',
            '[Author]': '[au]',
            '[Journal]': '[ta]'
        }
        
        for old, new in field_mappings.items():
            query = query.replace(old, new)
        
        if year_range and year_range[0] and year_range[1]:
            year_filter = f" AND ({year_range[0]}[pdat] : {year_range[1]}[pdat])"
            query = f"({query}){year_filter}"
        
        return query
    
    def fetch_pubmed_batch(self, ids_batch: list) -> list:
        """Obtiene detalles de un lote de IDs de PubMed"""
        results = []
        try:
            ids = ','.join(ids_batch)
            fetch_url = (
                f"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi"
                f"?db=pubmed"
                f"&id={ids}"
                f"&retmode=xml"
                f"&tool=streamlit_app"
                f"&email={self.email}"
            )
            
            time.sleep(self.delay)
            response = requests.get(fetch_url, timeout=30)
            response.raise_for_status()
            
            root = ET.fromstring(response.content)
            
            for article in root.findall('.//PubmedArticle'):
                try:
                    title_elem = article.find('.//ArticleTitle')
                    title = title_elem.text if title_elem is not None else "T√≠tulo no disponible"
                    
                    authors = []
                    author_list = article.findall('.//Author')
                    for author in author_list[:5]:
                        last = author.find('LastName')
                        fore = author.find('ForeName')
                        if last is not None and fore is not None:
                            authors.append(f"{fore.text} {last.text}")
                        elif last is not None:
                            authors.append(last.text)
                    
                    journal_elem = article.find('.//Title')
                    journal = journal_elem.text if journal_elem is not None else ""
                    
                    year_elem = article.find('.//PubDate/Year')
                    if year_elem is None:
                        year_elem = article.find('.//PubDate/MedlineDate')
                    year = year_elem.text[:4] if year_elem is not None and year_elem.text else ""
                    
                    doi_elem = article.find(".//ArticleId[@IdType='doi']")
                    doi = doi_elem.text if doi_elem is not None else ""
                    
                    pmid_elem = article.find(".//PMID")
                    pmid = pmid_elem.text if pmid_elem is not None else ""
                    
                    abstract_elem = article.find('.//AbstractText')
                    abstract = abstract_elem.text if abstract_elem is not None else ""
                    
                    results.append({
                        'base_datos': 'PubMed',
                        'titulo': title,
                        'autores': ', '.join(authors)[:200],
                        'revista': journal,
                        'a√±o': year,
                        'doi': doi,
                        'url': f"https://pubmed.ncbi.nlm.nih.gov/{pmid}/" if pmid else "",
                        'pmid': pmid,
                        'abstract': abstract[:500] + '...' if len(abstract) > 500 else abstract,
                        'tipo': 'Art√≠culo'
                    })
                    
                except Exception as e:
                    continue
                    
        except Exception as e:
            pass
        
        return results
    
    def search_pubmed_advanced(self, query: str, max_results: int = 1000, year_range: tuple = None) -> list:
        """B√∫squeda avanzada en PubMed con soporte para hasta 1000 resultados"""
        all_results = []
        try:
            formatted_query = self.format_pubmed_query(query, year_range)
            encoded_query = urllib.parse.quote(formatted_query)
            
            # Obtener conteo total
            count_url = (
                f"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi"
                f"?db=pubmed"
                f"&term={encoded_query}"
                f"&retmode=json"
                f"&retmax=0"
                f"&tool=streamlit_app"
                f"&email={self.email}"
            )
            
            response = requests.get(count_url, timeout=30)
            response.raise_for_status()
            count_data = response.json()
            total_count = int(count_data.get('esearchresult', {}).get('count', '0'))
            
            if total_count == 0:
                return []
            
            results_to_get = min(max_results, total_count)
            
            # Obtener IDs en lotes
            batch_size = 200
            all_ids = []
            
            for retstart in range(0, results_to_get, batch_size):
                search_url = (
                    f"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi"
                    f"?db=pubmed"
                    f"&term={encoded_query}"
                    f"&retmode=json"
                    f"&retmax={batch_size}"
                    f"&retstart={retstart}"
                    f"&sort=relevance"
                    f"&tool=streamlit_app"
                    f"&email={self.email}"
                )
                
                time.sleep(self.delay)
                response = requests.get(search_url, timeout=30)
                response.raise_for_status()
                search_data = response.json()
                
                id_list = search_data.get('esearchresult', {}).get('idlist', [])
                all_ids.extend(id_list)
                
                if len(id_list) < batch_size:
                    break
            
            # Obtener detalles en lotes
            if all_ids:
                batch_size_details = 100
                id_batches = [all_ids[i:i + batch_size_details] 
                             for i in range(0, len(all_ids), batch_size_details)]
                
                with ThreadPoolExecutor(max_workers=3) as executor:
                    futures = [executor.submit(self.fetch_pubmed_batch, batch) 
                              for batch in id_batches]
                    
                    for future in as_completed(futures):
                        batch_results = future.result()
                        all_results.extend(batch_results)
            
        except Exception as e:
            pass
        
        return all_results
    
    def search_crossref(self, query: str, max_results: int = 1000, year_range: tuple = None) -> list:
        """Busca en CrossRef con soporte para hasta 1000 resultados"""
        results = []
        try:
            simple_query = re.sub(r'"[^"]*"\[[^\]]*\]', '', query)
            simple_query = re.sub(r'[\(\)]', '', simple_query)
            simple_query = ' '.join(simple_query.split())
            
            url = "https://api.crossref.org/works"
            params = {
                'query': simple_query[:200],
                'rows': min(1000, max_results),
                'sort': 'relevance',
                'order': 'desc'
            }
            
            if year_range and year_range[0] and year_range[1]:
                params['filter'] = f"from-pub-date:{year_range[0]},until-pub-date:{year_range[1]}"
            
            time.sleep(self.delay)
            response = requests.get(url, params=params, timeout=30)
            response.raise_for_status()
            data = response.json()
            
            for item in data.get('message', {}).get('items', []):
                year = ''
                if item.get('published-print'):
                    year = str(item['published-print']['date-parts'][0][0])
                elif item.get('published-online'):
                    year = str(item['published-online']['date-parts'][0][0])
                
                authors = []
                for author in item.get('author', [])[:5]:
                    if 'given' in author and 'family' in author:
                        authors.append(f"{author['given']} {author['family']}")
                    elif 'family' in author:
                        authors.append(author['family'])
                
                results.append({
                    'base_datos': 'CrossRef',
                    'titulo': item.get('title', ['T√≠tulo no disponible'])[0],
                    'autores': ', '.join(authors)[:200],
                    'revista': item.get('container-title', [''])[0] if item.get('container-title') else '',
                    'a√±o': year,
                    'doi': item.get('DOI', ''),
                    'url': f"https://doi.org/{item['DOI']}" if item.get('DOI') else '',
                    'tipo': item.get('type', '')
                })
                
        except Exception as e:
            pass
        
        return results
    
    def search_openalex(self, query: str, max_results: int = 1000, year_range: tuple = None) -> list:
        """Busca en OpenAlex con soporte para hasta 1000 resultados"""
        results = []
        try:
            simple_query = re.sub(r'"[^"]*"\[[^\]]*\]', '', query)
            simple_query = re.sub(r'[\(\)]', '', simple_query)
            simple_query = ' '.join(simple_query.split())
            
            url = "https://api.openalex.org/works"
            params = {
                'search': simple_query[:200],
                'per-page': 200,  # M√°ximo permitido por OpenAlex
                'sort': 'relevance_score:desc'
            }
            
            if year_range and year_range[0] and year_range[1]:
                params['filter'] = f"publication_year:{year_range[0]}-{year_range[1]}"
            
            page = 1
            while len(results) < max_results:
                params['page'] = page
                
                time.sleep(self.delay)
                response = requests.get(url, params=params, timeout=30)
                response.raise_for_status()
                data = response.json()
                
                for item in data.get('results', []):
                    results.append({
                        'base_datos': 'OpenAlex',
                        'titulo': item.get('title', 'T√≠tulo no disponible'),
                        'autores': ', '.join([a.get('author', {}).get('display_name', '') 
                                             for a in item.get('authorships', [])[:5]]),
                        'revista': item.get('host_venue', {}).get('display_name', '') if item.get('host_venue') else '',
                        'a√±o': str(item.get('publication_year', '')),
                        'doi': item.get('doi', '').replace('https://doi.org/', ''),
                        'url': item.get('doi', ''),
                        'tipo': item.get('type', ''),
                        'open_access': item.get('open_access', {}).get('oa_url', '')
                    })
                
                if len(data.get('results', [])) < params['per-page']:
                    break
                    
                page += 1
                
        except Exception as e:
            pass
        
        return results[:max_results]
    
    def search_europe_pmc(self, query: str, max_results: int = 1000, year_range: tuple = None) -> list:
        """Busca en Europe PMC con soporte para hasta 1000 resultados"""
        results = []
        try:
            simple_query = re.sub(r'"[^"]*"\[[^\]]*\]', '', query)
            simple_query = re.sub(r'[\(\)]', '', simple_query)
            simple_query = ' '.join(simple_query.split())
            
            if year_range and year_range[0] and year_range[1]:
                date_filter = f" AND (PUB_YEAR:{year_range[0]}-{year_range[1]})"
                simple_query = f"({simple_query}){date_filter}"
            
            page = 1
            page_size = 100  # M√°ximo por p√°gina en Europe PMC
            
            while len(results) < max_results:
                url = "https://www.ebi.ac.uk/europepmc/webservices/rest/search"
                params = {
                    'query': simple_query[:500],
                    'format': 'json',
                    'pageSize': page_size,
                    'page': page,
                    'resultType': 'core'
                }
                
                time.sleep(self.delay)
                response = requests.get(url, params=params, timeout=30)
                response.raise_for_status()
                data = response.json()
                
                for item in data.get('resultList', {}).get('result', []):
                    results.append({
                        'base_datos': 'Europe PMC',
                        'titulo': item.get('title', 'T√≠tulo no disponible'),
                        'autores': ', '.join([a.get('fullName', '') for a in item.get('authorList', {}).get('author', [])[:5]]),
                        'revista': item.get('journalTitle', ''),
                        'a√±o': str(item.get('pubYear', '')),
                        'doi': item.get('doi', ''),
                        'url': f"https://europepmc.org/article/{item.get('source', '')}/{item.get('id', '')}",
                        'pmid': item.get('pmid', ''),
                        'pmcid': item.get('pmcid', ''),
                        'abstract': item.get('abstractText', '')[:300] + '...' if item.get('abstractText') else '',
                        'tipo': 'Art√≠culo'
                    })
                
                if len(data.get('resultList', {}).get('result', [])) < page_size:
                    break
                    
                page += 1
                
        except Exception as e:
            pass
        
        return results[:max_results]
    
    def search_all(self, query: str, max_results_per_db: int = 1000, selected_dbs: list = None, 
                   year_range: tuple = None) -> pd.DataFrame:
        """Busca en todas las bases de datos seleccionadas con alto volumen"""
        
        if selected_dbs is None:
            selected_dbs = ['PubMed', 'CrossRef', 'OpenAlex', 'Europe PMC']
        
        all_results = []
        
        search_functions = {
            'PubMed': self.search_pubmed_advanced,
            'CrossRef': self.search_crossref,
            'OpenAlex': self.search_openalex,
            'Europe PMC': self.search_europe_pmc
        }
        
        for db in selected_dbs:
            if db in search_functions:
                try:
                    results = search_functions[db](query, max_results_per_db, year_range)
                    all_results.extend(results)
                except Exception as e:
                    pass
        
        if all_results:
            df = pd.DataFrame(all_results)
            
            if 'a√±o' in df.columns:
                df['a√±o'] = pd.to_numeric(df['a√±o'], errors='coerce')
            
            if 'doi' in df.columns:
                df = df.drop_duplicates(subset=['doi'], keep='first')
            
            return df
        else:
            return pd.DataFrame()


# ============================================================================
# OBTENEDOR DE TEXTO DE ART√çCULOS (MEJORADO)
# ============================================================================

class ArticleTextFetcher:
    """Obtiene el texto completo de art√≠culos desde fuentes abiertas"""
    
    def __init__(self):
        self.session = requests.Session()
        retry_strategy = Retry(
            total=2,
            backoff_factor=0.5,
            status_forcelist=[429, 500, 502, 503, 504],
        )
        adapter = HTTPAdapter(max_retries=retry_strategy)
        self.session.mount("http://", adapter)
        self.session.mount("https://", adapter)
        
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        
        self.min_delay = 0.5
        self.last_request_time = 0
    
    def wait(self):
        """Control de tasa simple"""
        current_time = time.time()
        time_since_last = current_time - self.last_request_time
        
        if time_since_last < self.min_delay:
            time.sleep(self.min_delay - time_since_last)
        
        self.last_request_time = time.time()
    
    def get_text_from_doi(self, doi: str) -> Tuple[Optional[str], str]:
        """
        Obtiene el texto completo del art√≠culo usando el DOI
        Retorna (texto, fuente)
        """
        if not doi or pd.isna(doi) or doi == '':
            return None, "DOI vac√≠o"
        
        doi = str(doi).strip()
        if doi.startswith('https://doi.org/'):
            doi = doi.replace('https://doi.org/', '')
        elif doi.startswith('doi:'):
            doi = doi.replace('doi:', '')
        
        self.wait()
        
        # Intentar 1: OpenAlex (para obtener URLs Open Access)
        try:
            url = f"https://api.openalex.org/works/doi:{doi}"
            response = self.session.get(url, timeout=10)
            if response.status_code == 200:
                data = response.json()
                oa_url = data.get('open_access', {}).get('oa_url')
                if oa_url:
                    return f"Open Access disponible en: {oa_url}", f"OpenAlex (OA URL)"
        except:
            pass
        
        # Intentar 2: Europe PMC (tienen abstracts y metadata)
        try:
            url = f"https://www.ebi.ac.uk/europepmc/webservices/rest/search?query=DOI:{doi}&format=json"
            response = self.session.get(url, timeout=10)
            if response.status_code == 200:
                data = response.json()
                results = data.get('resultList', {}).get('result', [])
                if results:
                    abstract = results[0].get('abstractText', '')
                    if abstract:
                        return abstract, "Europe PMC (Abstract)"
        except:
            pass
        
        # Intentar 3: Unpaywall
        try:
            url = f"https://api.unpaywall.org/v2/{doi}?email=usuario@ejemplo.com"
            response = self.session.get(url, timeout=10)
            if response.status_code == 200:
                data = response.json()
                if data.get('is_oa') and data.get('best_oa_location'):
                    pdf_url = data['best_oa_location'].get('url_for_pdf')
                    if pdf_url:
                        return f"PDF Open Access en: {pdf_url}", "Unpaywall"
        except:
            pass
        
        # Intentar 4: PubMed (si es art√≠culo de PubMed)
        try:
            # Extraer PMID si existe en el DOI? No, mejor buscar por DOI
            url = f"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&term={doi}[DOI]&retmode=json"
            response = self.session.get(url, timeout=10)
            if response.status_code == 200:
                data = response.json()
                id_list = data.get('esearchresult', {}).get('idlist', [])
                if id_list:
                    pmid = id_list[0]
                    fetch_url = f"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id={pmid}&retmode=xml"
                    fetch_response = self.session.get(fetch_url, timeout=10)
                    if fetch_response.status_code == 200:
                        root = ET.fromstring(fetch_response.content)
                        abstract_elem = root.find('.//AbstractText')
                        if abstract_elem is not None and abstract_elem.text:
                            return abstract_elem.text, "PubMed (Abstract)"
        except:
            pass
        
        return None, "No se pudo obtener texto completo"


# ============================================================================
# CLASE PRINCIPAL INTEGRADA (ALTO VOLUMEN)
# ============================================================================

class IntegratedScientificVerifier:
    """
    Clase principal que integra b√∫squeda y verificaci√≥n sem√°ntica
    Soporta hasta 1000 art√≠culos por base de datos
    """
    
    def __init__(self, email: str):
        self.search_engine = ScientificSearchEngine(email)
        self.semantic_verifier = AdvancedSemanticVerifier()
        self.text_fetcher = ArticleTextFetcher()
        self.results = []
        self.stats = {
            'total_articles': 0,
            'analyzed': 0,
            'with_text': 0,
            'corroboran': 0,
            'contradicen': 0,
            'inconclusos': 0
        }
    
    def run_analysis(self, query: str, hypothesis: str, max_results_per_db: int = 1000, 
                     selected_dbs: list = None, year_range: tuple = None,
                     progress_callback=None) -> pd.DataFrame:
        """
        Ejecuta el flujo completo: b√∫squeda + an√°lisis sem√°ntico
        """
        self.results = []
        self.stats = {
            'total_articles': 0,
            'analyzed': 0,
            'with_text': 0,
            'corroboran': 0,
            'contradicen': 0,
            'inconclusos': 0
        }
        
        # PASO 1: B√öSQUEDA
        if progress_callback:
            progress_callback("üîç Buscando art√≠culos en bases de datos...", 0.05)
        
        articles_df = self.search_engine.search_all(
            query, max_results_per_db, selected_dbs, year_range
        )
        
        if articles_df.empty:
            return pd.DataFrame()
        
        self.stats['total_articles'] = len(articles_df)
        
        if progress_callback:
            progress_callback(f"‚úÖ Encontrados {len(articles_df)} art√≠culos. Iniciando an√°lisis...", 0.1)
        
        # PASO 2: AN√ÅLISIS DE CADA ART√çCULO
        results_list = []
        
        # Procesar en lotes para mejor manejo de memoria
        batch_size = 50
        total_articles = len(articles_df)
        
        for batch_start in range(0, total_articles, batch_size):
            batch_end = min(batch_start + batch_size, total_articles)
            batch_df = articles_df.iloc[batch_start:batch_end]
            
            for idx, row in batch_df.iterrows():
                current = idx + 1
                if progress_callback:
                    progress_value = 0.1 + 0.85 * (current / total_articles)
                    progress_callback(
                        f"üî¨ Analizando art√≠culo {current}/{total_articles}: {str(row['titulo'])[:50]}...", 
                        progress_value
                    )
                
                # Obtener texto del art√≠culo
                article_text, source = self.text_fetcher.get_text_from_doi(row.get('doi', ''))
                
                result_row = {
                    'base_datos': row.get('base_datos', 'Desconocida'),
                    'titulo': row.get('titulo', 'Sin t√≠tulo'),
                    'autores': row.get('autores', ''),
                    'revista': row.get('revista', ''),
                    'a√±o': row.get('a√±o', ''),
                    'doi': row.get('doi', ''),
                    'url': row.get('url', ''),
                    'texto_disponible': article_text is not None,
                    'fuente_texto': source if article_text else 'No disponible',
                    'veredicto': '',
                    'confianza': 0,
                    'puntuacion': 0,
                    'evidencia_a_favor': 0,
                    'evidencia_en_contra': 0,
                    'oraciones_totales': 0,
                    'oraciones_relevantes': 0,
                    'detalle_evidencia': ''
                }
                
                if article_text:
                    self.stats['with_text'] += 1
                    
                    # Analizar sem√°nticamente
                    analysis = self.semantic_verifier.verify_article_text(article_text, hypothesis)
                    
                    if analysis['success']:
                        verdict = analysis['verdict']
                        
                        result_row.update({
                            'veredicto': verdict['verdict_text'],
                            'confianza': verdict['confidence'],
                            'puntuacion': verdict['score'],
                            'evidencia_a_favor': verdict['support_count'],
                            'evidencia_en_contra': verdict['against_count'],
                            'oraciones_totales': analysis['total_sentences'],
                            'oraciones_relevantes': analysis['relevant_sentences']
                        })
                        
                        # Actualizar estad√≠sticas
                        self.stats['analyzed'] += 1
                        if 'CORROBORA' in verdict['verdict_text']:
                            self.stats['corroboran'] += 1
                        elif 'CONTRADICE' in verdict['verdict_text']:
                            self.stats['contradicen'] += 1
                        else:
                            self.stats['inconclusos'] += 1
                        
                        # Guardar evidencia resumida
                        if analysis['evidence']:
                            ev_summary = []
                            for ev in analysis['evidence'][:3]:
                                ev_summary.append(f"[{ev['section']}] {ev['relation']} ({ev['certainty']})")
                            result_row['detalle_evidencia'] = ' | '.join(ev_summary)
                else:
                    result_row['veredicto'] = 'TEXTO NO DISPONIBLE'
                
                results_list.append(result_row)
            
            # Peque√±a pausa entre lotes
            time.sleep(1)
        
        if progress_callback:
            progress_callback("‚úÖ An√°lisis completado", 1.0)
        
        self.results = pd.DataFrame(results_list)
        return self.results
    
    def generate_report(self) -> str:
        """Genera un reporte textual de los resultados"""
        if self.results.empty:
            return "No hay resultados para generar reporte."
        
        report = []
        report.append("="*80)
        report.append("REPORTE DE VERIFICACI√ìN SEM√ÅNTICA INTEGRADA")
        report.append("="*80)
        report.append(f"Fecha: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append(f"Total art√≠culos encontrados: {self.stats['total_articles']}")
        report.append(f"Art√≠culos con texto disponible: {self.stats['with_text']}")
        report.append(f"Art√≠culos analizados: {self.stats['analyzed']}")
        report.append("")
        report.append("RESULTADOS GLOBALES:")
        report.append(f"‚úÖ Corroboran: {self.stats['corroboran']}")
        report.append(f"‚ùå Contradicen: {self.stats['contradicen']}")
        report.append(f"‚ö†Ô∏è Inconclusos: {self.stats['inconclusos']}")
        report.append("")
        report.append("DETALLE POR ART√çCULO:")
        report.append("-"*80)
        
        for idx, row in self.results.iterrows():
            report.append(f"\nüìÑ {row['titulo']}")
            report.append(f"   Base: {row['base_datos']} | A√±o: {row['a√±o']}")
            report.append(f"   DOI: {row['doi']}")
            
            if row['veredicto'] == 'TEXTO NO DISPONIBLE':
                report.append(f"   ‚ö†Ô∏è {row['veredicto']} - {row['fuente_texto']}")
            else:
                report.append(f"   Veredicto: {row['veredicto']} (Confianza: {row['confianza']:.1%})")
                report.append(f"   Evidencia: {row['evidencia_a_favor']} a favor, {row['evidencia_en_contra']} en contra")
                if row['detalle_evidencia']:
                    report.append(f"   Evidencia destacada: {row['detalle_evidencia']}")
        
        return "\n".join(report)
    
    def generate_html_report(self) -> str:
        """Genera un reporte HTML de los resultados para enviar por correo"""
        if self.results.empty:
            return "<p>No hay resultados para generar reporte.</p>"
        
        html = f"""
        <!DOCTYPE html>
        <html>
        <head>
            <meta charset="UTF-8">
            <style>
                body {{ font-family: Arial, sans-serif; margin: 20px; }}
                h1 {{ color: #1E88E5; }}
                h2 {{ color: #333; border-bottom: 2px solid #1E88E5; padding-bottom: 5px; }}
                .stats {{ background-color: #f5f5f5; padding: 15px; border-radius: 5px; }}
                .verdict-assert {{ color: #4CAF50; font-weight: bold; }}
                .verdict-reject {{ color: #f44336; font-weight: bold; }}
                .verdict-inconclusive {{ color: #ff9800; font-weight: bold; }}
                table {{ border-collapse: collapse; width: 100%; }}
                th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}
                th {{ background-color: #1E88E5; color: white; }}
                tr:nth-child(even) {{ background-color: #f2f2f2; }}
            </style>
        </head>
        <body>
            <h1>üî¨ Reporte de Verificaci√≥n Sem√°ntica</h1>
            <p><strong>Fecha:</strong> {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
            
            <h2>üìä Resultados Globales</h2>
            <div class="stats">
                <p><strong>Total art√≠culos encontrados:</strong> {self.stats['total_articles']}</p>
                <p><strong>Art√≠culos con texto disponible:</strong> {self.stats['with_text']}</p>
                <p><strong>Art√≠culos analizados:</strong> {self.stats['analyzed']}</p>
                <p><strong>‚úÖ Corroboran:</strong> {self.stats['corroboran']}</p>
                <p><strong>‚ùå Contradicen:</strong> {self.stats['contradicen']}</p>
                <p><strong>‚ö†Ô∏è Inconclusos:</strong> {self.stats['inconclusos']}</p>
            </div>
            
            <h2>üìã Detalle de Art√≠culos</h2>
            <table>
                <tr>
                    <th>Base</th>
                    <th>T√≠tulo</th>
                    <th>A√±o</th>
                    <th>Veredicto</th>
                    <th>Confianza</th>
                </tr>
        """
        
        for idx, row in self.results.iterrows():
            # Manejar valores nulos o vac√≠os
            titulo = str(row.get('titulo', '')) if pd.notna(row.get('titulo')) else "T√≠tulo no disponible"
            base_datos = str(row.get('base_datos', '')) if pd.notna(row.get('base_datos')) else "Desconocida"
            a√±o = str(row.get('a√±o', '')) if pd.notna(row.get('a√±o')) else ""
            veredicto = str(row.get('veredicto', '')) if pd.notna(row.get('veredicto')) else "NO DISPONIBLE"
            confianza = float(row.get('confianza', 0)) if pd.notna(row.get('confianza')) else 0
            
            verdict_class = ""
            if 'CORROBORA' in veredicto:
                verdict_class = "verdict-assert"
            elif 'CONTRADICE' in veredicto:
                verdict_class = "verdict-reject"
            elif 'NO CONCLUYENTE' in veredicto:
                verdict_class = "verdict-inconclusive"
            
            html += f"""
                <tr>
                    <td>{base_datos}</td>
                    <td>{titulo[:100]}...</td>
                    <td>{a√±o}</td>
                    <td class="{verdict_class}">{veredicto}</td>
                    <td>{confianza:.1%}</td>
                </tr>
            """
        
        html += """
            </table>
        </body>
        </html>
        """
        
        return html


# ============================================================================
# FUNCIONES DE VISUALIZACI√ìN
# ============================================================================

def get_badge_class(db_name: str) -> str:
    """Devuelve la clase CSS para el badge de base de datos"""
    classes = {
        'PubMed': 'badge-pubmed',
        'CrossRef': 'badge-crossref',
        'OpenAlex': 'badge-openalex',
        'Semantic Scholar': 'badge-semantic',
        'DOAJ': 'badge-doaj',
        'Europe PMC': 'badge-europepmc'
    }
    return classes.get(db_name, 'badge-pubmed')

def get_verdict_class(verdict: str) -> str:
    """Devuelve la clase CSS para el veredicto"""
    if 'CORROBORA' in verdict:
        return 'verdict-assert'
    elif 'CONTRADICE' in verdict:
        return 'verdict-reject'
    elif 'NO CONCLUYENTE' in verdict:
        return 'verdict-inconclusive'
    else:
        return ''


# ============================================================================
# FUNCI√ìN PARA ENVIAR RESULTADOS POR CORREO
# ============================================================================

def enviar_resultados_email(destinatario, integrator):
    """Env√≠a los resultados del an√°lisis por correo electr√≥nico"""
    
    if integrator is None or integrator.results is None or integrator.results.empty:
        st.warning("No hay resultados para enviar por correo.")
        return False
    
    # Generar reportes
    reporte_txt = integrator.generate_report()
    reporte_html = integrator.generate_html_report()
    
    # Preparar archivos adjuntos
    archivos = []
    
    # CSV
    csv_buffer = io.BytesIO()
    integrator.results.to_csv(csv_buffer, index=False, encoding='utf-8')
    archivos.append({
        'nombre': f"resultados_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
        'contenido': csv_buffer.getvalue()
    })
    
    # Excel
    excel_buffer = io.BytesIO()
    with pd.ExcelWriter(excel_buffer, engine='openpyxl') as writer:
        integrator.results.to_excel(writer, sheet_name='Resultados', index=False)
        summary = pd.DataFrame([integrator.stats])
        summary.to_excel(writer, sheet_name='Resumen', index=False)
    archivos.append({
        'nombre': f"resultados_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx",
        'contenido': excel_buffer.getvalue()
    })
    
    # Asunto y mensaje
    asunto = f"üî¨ Reporte de Verificaci√≥n Sem√°ntica - {datetime.now().strftime('%Y-%m-%d %H:%M')}"
    
    mensaje_html = f"""
    <html>
    <body>
        <h2>üî¨ Reporte de Verificaci√≥n Sem√°ntica</h2>
        <p>Estimado usuario,</p>
        <p>Adjunto encontrar√°s los resultados completos de tu an√°lisis de verificaci√≥n sem√°ntica.</p>
        
        <h3>üìä Resumen</h3>
        <ul>
            <li><strong>Total art√≠culos:</strong> {integrator.stats['total_articles']}</li>
            <li><strong>Art√≠culos analizados:</strong> {integrator.stats['analyzed']}</li>
            <li><strong>‚úÖ Corroboran:</strong> {integrator.stats['corroboran']}</li>
            <li><strong>‚ùå Contradicen:</strong> {integrator.stats['contradicen']}</li>
            <li><strong>‚ö†Ô∏è Inconclusos:</strong> {integrator.stats['inconclusos']}</li>
        </ul>
        
        <p>Puedes encontrar el detalle completo en los archivos adjuntos.</p>
        
        <hr>
        <p style="color: #666; font-size: 0.9em;">
            Este es un mensaje autom√°tico del Buscador y Verificador Sem√°ntico Integrado.
        </p>
    </body>
    </html>
    """
    
    mensaje_texto = f"""
    Reporte de Verificaci√≥n Sem√°ntica
    
    Resumen:
    - Total art√≠culos: {integrator.stats['total_articles']}
    - Art√≠culos analizados: {integrator.stats['analyzed']}
    - ‚úÖ Corroboran: {integrator.stats['corroboran']}
    - ‚ùå Contradicen: {integrator.stats['contradicen']}
    - ‚ö†Ô∏è Inconclusos: {integrator.stats['inconclusos']}
    
    Los archivos adjuntos contienen el detalle completo.
    """
    
    # Enviar correo
    return enviar_correo(
        destinatario=destinatario,
        asunto=asunto,
        mensaje_html=mensaje_html,
        mensaje_texto=mensaje_texto,
        archivos=archivos
    )


# ============================================================================
# INTERFAZ PRINCIPAL DE STREAMLIT
# ============================================================================

def main():
    """Funci√≥n principal de la aplicaci√≥n"""
    
    st.markdown('<h1 class="main-header">üî¨ Buscador y Verificador Sem√°ntico Integrado</h1>', unsafe_allow_html=True)
    st.markdown('<p class="sub-header">ALTO VOLUMEN: Hasta 1000 art√≠culos por base ‚Ä¢ An√°lisis AI autom√°tico</p>', 
                unsafe_allow_html=True)
    
    # Inicializar asistente de hip√≥tesis
    hypothesis_assistant = HypothesisAssistant()
    
    # Renderizar asistente
    hypothesis_assistant.render_assistant_ui()
    
    # Sidebar
    with st.sidebar:
        st.image("https://img.icons8.com/fluency/96/000000/artificial-intelligence.png", width=80)
        st.markdown("## ‚öôÔ∏è Configuraci√≥n")
        
        email = st.text_input("üìß Email (requerido para NCBI y para recibir resultados)", 
                              value="usuario@ejemplo.com",
                              help="Este email se usar√° para NCBI y para enviarte los resultados")
        
        # Estado de NLTK
        if NLTK_READY:
            st.success("‚úÖ NLTK configurado correctamente")
        else:
            st.warning("‚ö†Ô∏è Usando tokenizador alternativo")
        
        st.markdown("### üìö Bases de Datos")
        col1, col2 = st.columns(2)
        with col1:
            pubmed = st.checkbox('PubMed', value=True)
            crossref = st.checkbox('CrossRef', value=True)
            openalex = st.checkbox('OpenAlex', value=True)
        with col2:
            semantic = st.checkbox('Semantic Scholar', value=False)
            doaj = st.checkbox('DOAJ', value=False)
            europepmc = st.checkbox('Europe PMC', value=True)
        
        databases = {
            'PubMed': pubmed,
            'CrossRef': crossref,
            'OpenAlex': openalex,
            'Semantic Scholar': semantic,
            'DOAJ': doaj,
            'Europe PMC': europepmc
        }
        
        st.markdown("---")
        
        st.markdown("### üìÖ Filtro de A√±os")
        col1, col2 = st.columns(2)
        with col1:
            year_from = st.number_input("Desde", min_value=1900, max_value=2025, value=2015)
        with col2:
            year_to = st.number_input("Hasta", min_value=1900, max_value=2025, value=2025)
        
        year_range = (year_from, year_to) if year_from < year_to else None
        
        st.markdown("### üìä Resultados por base")
        max_results = st.slider(
            "M√°ximo resultados por base:", 
            min_value=100, max_value=1000, value=500, step=100,
            help="Hasta 1000 art√≠culos por base de datos"
        )
        
        st.markdown("### üîß Umbrales de an√°lisis")
        min_relevance = st.slider(
            "Relevancia m√≠nima",
            min_value=0.05,
            max_value=0.3,
            value=0.1,
            step=0.05,
            help="M√≠nimo de coincidencia con t√©rminos de la hip√≥tesis"
        )
        
        st.markdown("### ‚ö†Ô∏è Advertencia")
        st.warning("""
        **Tiempo de procesamiento:**
        - 500 art√≠culos: ~10-15 minutos
        - 1000 art√≠culos: ~20-30 minutos
        - Depende de disponibilidad de texto completo
        """)
        
        st.markdown("### üìã Ejemplos")
        if st.button("Cargar ejemplo: Ticagrelor y disnea"):
            st.session_state['query'] = '((("Ticagrelor"[Mesh]) OR (ticagrelor)) AND ((((((((("Myocardial Ischemia"[Mesh]) OR ("Acute Coronary Syndrome"[Mesh])) OR ("Angina Pectoris"[Mesh])) OR ("Coronary Disease"[Mesh])) OR ("Coronary Artery Disease"[Mesh])) OR ("Kounis Syndrome"[Mesh])) OR ("Myocardial Infarction"[Mesh])) OR ("Myocardial Reperfusion Injury"[Mesh])) OR (((((((((MYOCARDIAL ISCHEMIA) OR (ACUTE CORONARY SYNDROME)) OR (ANGINA PECTORIS)) OR (CORONARY DISEASE)) OR (CORONARY ARTERY DISEASE)) OR (kounis syndrome)) OR (myocardial infarction)) OR (myocardial reperfusion injury)) OR (ischemic heart disease)))) AND ((((((cohort studies) OR (prospective studies)) OR ("prospective clinical trial")) OR ("clinical records")) OR (randomized clinical trial)) OR ((("Clinical Study" [Publication Type] OR "Observational Study" [Publication Type]) OR "Retrospective Studies"[Mesh]) OR "Randomized Controlled Trial" [Publication Type]))) AND (adults or adult)'
            st.session_state['hypothesis'] = "El ticagrelor causa disnea como efecto secundario en pacientes con cardiopat√≠a isqu√©mica"
            st.session_state['hypothesis_en'] = "Ticagrelor causes dyspnea as a side effect in patients with ischemic heart disease"
        
        if st.button("Cargar ejemplo: COVID-19"):
            st.session_state['query'] = '("COVID-19"[Mesh] AND "Myocardial Injury"[Mesh])'
            st.session_state['hypothesis'] = "La infecci√≥n por COVID-19 causa da√±o mioc√°rdico"
            st.session_state['hypothesis_en'] = "COVID-19 infection causes myocardial injury"
    
    # √Årea principal
    col1, col2 = st.columns([2, 1])
    
    with col1:
        search_query = st.text_area(
            "üîç Consulta de b√∫squeda:",
            value=st.session_state.get('query', ''),
            height=120,
            placeholder='Ej: (("Ticagrelor"[Mesh]) AND ("Myocardial Ischemia"[Mesh]) AND ("dyspnea"))',
            help="Puedes usar t√©rminos MeSH o palabras clave"
        )
    
    with col2:
        hypothesis_es = st.text_area(
            "üî¨ Conjetura a verificar (espa√±ol):",
            value=st.session_state.get('hypothesis', ''),
            height=60,
            placeholder='Ej: El f√°rmaco X causa efecto Y en pacientes con Z',
            help="Escribe tu hip√≥tesis en espa√±ol"
        )
        
        # Mostrar traducci√≥n autom√°tica
        if hypothesis_es:
            if 'hypothesis_en' not in st.session_state or not st.session_state['hypothesis_en']:
                translator = TranslationManager()
                hypothesis_en = translator.translate_to_english(hypothesis_es)
                st.session_state['hypothesis_en'] = hypothesis_en
            
            st.markdown(f"**üá¨üáß Ingl√©s (para b√∫squeda):**")
            st.info(st.session_state.get('hypothesis_en', ''))
        else:
            # Si hay hip√≥tesis en ingl√©s guardada, mostrarla
            if 'hypothesis_en' in st.session_state and st.session_state['hypothesis_en']:
                st.markdown(f"**üá¨üáß Ingl√©s (para b√∫squeda):**")
                st.info(st.session_state['hypothesis_en'])
    
    col1, col2, col3 = st.columns(3)
    with col2:
        analyze_button = st.button("üöÄ INICIAR AN√ÅLISIS INTEGRADO", type="primary", use_container_width=True)
    
    # Variable para guardar el integrador despu√©s del an√°lisis
    if 'integrator' not in st.session_state:
        st.session_state.integrator = None
    
    if analyze_button and search_query and hypothesis_es:
        if email and email != "usuario@ejemplo.com" and validate_email(email):
            # Inicializar integrador
            integrator = IntegratedScientificVerifier(email)
            integrator.semantic_verifier.UMBRAL_RELEVANCIA = min_relevance
            
            selected_dbs = [db for db, selected in databases.items() if selected]
            
            if not selected_dbs:
                st.error("‚ùå Selecciona al menos una base de datos")
                return
            
            # Usar la hip√≥tesis en ingl√©s para el an√°lisis
            hypothesis_for_analysis = st.session_state.get('hypothesis_en', hypothesis_es)
            
            # Contenedores para progreso
            progress_container = st.container()
            with progress_container:
                st.markdown('<div class="progress-container">', unsafe_allow_html=True)
                progress_bar = st.progress(0)
                status_text = st.empty()
                time_estimate = st.empty()
                st.markdown('</div>', unsafe_allow_html=True)
            
            # Funci√≥n de callback para actualizar progreso
            start_time = time.time()
            
            def update_progress(message, value):
                status_text.text(message)
                progress_bar.progress(value)
                elapsed = time.time() - start_time
                if value > 0:
                    estimated_total = elapsed / value
                    remaining = estimated_total * (1 - value)
                    if remaining > 0:
                        time_estimate.text(f"‚è±Ô∏è Tiempo transcurrido: {elapsed:.1f}s | Estimado restante: {remaining:.1f}s")
            
            # Ejecutar an√°lisis
            with st.spinner("Ejecutando an√°lisis integrado..."):
                results_df = integrator.run_analysis(
                    search_query, 
                    hypothesis_for_analysis, 
                    max_results, 
                    selected_dbs, 
                    year_range,
                    update_progress
                )
                elapsed_time = time.time() - start_time
            
            if not results_df.empty:
                st.success(f"‚úÖ An√°lisis completado en {elapsed_time:.1f} segundos ({elapsed_time/60:.1f} minutos)")
                
                # Guardar integrador en session state
                st.session_state.integrator = integrator
                
                # ESTAD√çSTICAS GLOBALES
                st.markdown("## üìä RESULTADOS GLOBALES")
                
                col1, col2, col3, col4, col5 = st.columns(5)
                with col1:
                    st.metric("Art√≠culos encontrados", integrator.stats['total_articles'])
                with col2:
                    st.metric("Con texto", integrator.stats['with_text'])
                with col3:
                    st.metric("‚úÖ Corroboran", integrator.stats['corroboran'])
                with col4:
                    st.metric("‚ùå Contradicen", integrator.stats['contradicen'])
                with col5:
                    st.metric("‚ö†Ô∏è Inconclusos", integrator.stats['inconclusos'])
                
                # GR√ÅFICOS
                col1, col2 = st.columns(2)
                
                with col1:
                    # Distribuci√≥n por base de datos
                    db_counts = results_df['base_datos'].value_counts().reset_index()
                    db_counts.columns = ['base_datos', 'count']
                    fig = px.bar(
                        db_counts, x='base_datos', y='count',
                        title=f"Art√≠culos por Base de Datos (Total: {integrator.stats['total_articles']})",
                        color='base_datos',
                        color_discrete_map={
                            'PubMed': '#1E88E5',
                            'CrossRef': '#43A047',
                            'OpenAlex': '#FDD835',
                            'Semantic Scholar': '#E53935',
                            'DOAJ': '#9C27B0',
                            'Europe PMC': '#FF5722'
                        }
                    )
                    fig.update_layout(showlegend=False)
                    st.plotly_chart(fig, use_container_width=True)
                
                with col2:
                    # Distribuci√≥n de veredictos (solo analizados)
                    analyzed_df = results_df[results_df['veredicto'] != 'TEXTO NO DISPONIBLE']
                    if not analyzed_df.empty:
                        verdict_counts = analyzed_df['veredicto'].value_counts().reset_index()
                        verdict_counts.columns = ['veredicto', 'count']
                        fig = px.pie(
                            verdict_counts, values='count', names='veredicto',
                            title=f"Distribuci√≥n de Veredictos (n={len(analyzed_df)})",
                            color_discrete_sequence=['#4CAF50', '#f44336', '#ff9800']
                        )
                        st.plotly_chart(fig, use_container_width=True)
                
                # TABLA DE RESULTADOS
                st.markdown("## üìã ART√çCULOS ANALIZADOS")
                
                # Filtrar para mostrar
                display_cols = ['base_datos', 'titulo', 'a√±o', 'veredicto', 'confianza', 
                               'evidencia_a_favor', 'evidencia_en_contra']
                display_df = results_df[display_cols].copy()
                display_df['confianza'] = display_df['confianza'].apply(lambda x: f"{x:.1%}" if x > 0 else 'N/A')
                
                st.dataframe(
                    display_df,
                    use_container_width=True,
                    height=400,
                    column_config={
                        'base_datos': 'Base',
                        'titulo': 'T√≠tulo',
                        'a√±o': 'A√±o',
                        'veredicto': 'Veredicto',
                        'confianza': 'Confianza',
                        'evidencia_a_favor': 'A favor',
                        'evidencia_en_contra': 'En contra'
                    }
                )
                
                # DETALLE POR ART√çCULO (limitado para no saturar)
                st.markdown("## üîç DETALLE DE AN√ÅLISIS (primeros 20 art√≠culos)")
                
                for idx, row in results_df.head(20).iterrows():
                    badge_class = get_badge_class(row['base_datos'])
                    
                    if row['veredicto'] == 'TEXTO NO DISPONIBLE':
                        st.markdown(f"""
                        <div class="result-card">
                            <span class="{badge_class}">{row['base_datos']}</span>
                            <div class="result-title">{row['titulo']}</div>
                            <div class="result-meta">
                                <b>Autores:</b> {row.get('autores', 'No disponible')[:150]}<br>
                                <b>A√±o:</b> {row.get('a√±o', 'No disponible')}<br>
                                <b>DOI:</b> {row.get('doi', 'No disponible')}<br>
                                <b>Estado:</b> ‚ö†Ô∏è TEXTO NO DISPONIBLE - {row['fuente_texto']}
                            </div>
                        """, unsafe_allow_html=True)
                    else:
                        verdict_class = get_verdict_class(row['veredicto'])
                        
                        st.markdown(f"""
                        <div class="result-card">
                            <span class="{badge_class}">{row['base_datos']}</span>
                            <div class="result-title">{row['titulo']}</div>
                            <div class="result-meta">
                                <b>Autores:</b> {row.get('autores', 'No disponible')[:150]}<br>
                                <b>A√±o:</b> {row.get('a√±o', 'No disponible')}<br>
                                <b>DOI:</b> {row.get('doi', 'No disponible')}<br>
                                <span class="{verdict_class}">{row['veredicto']}</span> (Confianza: {row['confianza']:.1%})<br>
                                <b>Evidencia:</b> {row['evidencia_a_favor']} a favor, {row['evidencia_en_contra']} en contra
                            </div>
                        """, unsafe_allow_html=True)
                        
                        if row['detalle_evidencia']:
                            st.markdown(f"""
                            <div class="evidence-box">
                                <b>Evidencia destacada:</b> {row['detalle_evidencia']}
                            </div>
                            """, unsafe_allow_html=True)
                    
                    col1, col2 = st.columns([1, 5])
                    with col1:
                        if row.get('url') and pd.notna(row['url']):
                            st.link_button("üîó Ver", row['url'])
                    with col2:
                        if row.get('doi') and pd.notna(row['doi']):
                            doi_link = f"https://doi.org/{row['doi']}"
                            st.link_button("üìã DOI", doi_link)
                    
                    st.markdown("</div>", unsafe_allow_html=True)
                
                if len(results_df) > 20:
                    st.info(f"... y {len(results_df) - 20} art√≠culos m√°s. Exporta los resultados para ver el listado completo.")
                
                # REPORTE Y EXPORTACI√ìN
                st.markdown("## üíæ EXPORTAR RESULTADOS")
                
                col1, col2, col3 = st.columns(3)
                
                # Reporte de texto
                report_text = integrator.generate_report()
                with col1:
                    st.download_button(
                        "üìù Reporte TXT",
                        report_text,
                        file_name=f"reporte_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt",
                        mime="text/plain",
                        use_container_width=True
                    )
                
                # CSV
                with col2:
                    csv = results_df.to_csv(index=False).encode('utf-8')
                    st.download_button(
                        "üìä CSV",
                        csv,
                        file_name=f"resultados_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                        mime="text/csv",
                        use_container_width=True
                    )
                
                # Excel
                with col3:
                    buffer = io.BytesIO()
                    with pd.ExcelWriter(buffer, engine='openpyxl') as writer:
                        results_df.to_excel(writer, sheet_name='Resultados', index=False)
                        
                        summary = pd.DataFrame([integrator.stats])
                        summary.to_excel(writer, sheet_name='Resumen', index=False)
                        
                        if not results_df[results_df['veredicto'] != 'TEXTO NO DISPONIBLE'].empty:
                            verdict_stats = results_df[results_df['veredicto'] != 'TEXTO NO DISPONIBLE']['veredicto'].value_counts().reset_index()
                            verdict_stats.columns = ['Veredicto', 'Cantidad']
                            verdict_stats.to_excel(writer, sheet_name='Por Veredicto', index=False)
                    
                    st.download_button(
                        "üì• Excel",
                        buffer.getvalue(),
                        file_name=f"resultados_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx",
                        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                        use_container_width=True
                    )
            
            else:
                st.warning("üòï No se encontraron art√≠culos. Prueba con otra consulta o ampl√≠a el rango de a√±os.")
        
        else:
            st.warning("‚ö†Ô∏è Ingresa un email v√°lido en la barra lateral")
    
    # Secci√≥n para enviar resultados por correo (despu√©s del an√°lisis)
    if st.session_state.integrator is not None and not st.session_state.integrator.results.empty:
        st.markdown("---")
        st.markdown("## üìß ENVIAR RESULTADOS POR CORREO")
        st.markdown('<div class="email-box">', unsafe_allow_html=True)
        
        col1, col2, col3 = st.columns([1, 2, 1])
        with col2:
            if st.button("üì® ENVIAR RESULTADOS A MI CORREO", type="primary", use_container_width=True):
                with st.spinner("Enviando resultados por correo..."):
                    if enviar_resultados_email(email, st.session_state.integrator):
                        st.success(f"‚úÖ Resultados enviados correctamente a {email}")
                        st.balloons()
                    else:
                        st.error("‚ùå No se pudo enviar el correo. Verifica la configuraci√≥n.")
        
        st.markdown('</div>', unsafe_allow_html=True)
    
    st.markdown("---")
    st.markdown("""
    <div style='text-align: center; color: #666; padding: 1rem;'>
        <p>üî¨ Buscador y Verificador Sem√°ntico Integrado v2.0 | ALTO VOLUMEN: Hasta 1000 art√≠culos por base | An√°lisis AI autom√°tico</p>
    </div>
    """, unsafe_allow_html=True)


if __name__ == "__main__":
    main()
